{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, num_embeddings, embedding_dim, device):\n",
    "        super().__init__(num_embeddings, embedding_dim, padding_idx=0, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert (\n",
    "            embedding_dim % 2 == 0\n",
    "        ), \"Embedding dimension must be even under this implementation\"\n",
    "        \n",
    "        self.encoding=torch.zeros(max_len, embedding_dim, device=device)\n",
    "        self.encoding.requires_grad = False\n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "        _2i = torch.arange(0, embedding_dim, step=2, device=device).float()\n",
    "        self.encoding[:, 0::2] = torch.sin(pos/10000**(_2i/embedding_dim))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos/10000**(_2i/embedding_dim)) # This is why positional embeding should be a even number\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, max_len, dropout, device):\n",
    "        super().__init__()\n",
    "        self.token_embs = TokenEmbedding(num_embeddings, embedding_dim, device)\n",
    "        self.pos_emb = PositionalEmbedding(embedding_dim, max_len, device)\n",
    "        self.drop_out = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        token_embs = self.token_embs(x)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        return self.drop_out(token_embs + pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 1, 2, 3, 4]], device='cuda:0')\n",
    "\n",
    "transformer_emb = TransformerEmbedding(5, 8, 10, dropout=0.1, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  1.1111,  0.0000,  1.1111,  0.0000,  1.1111,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.6528, -0.1120, -0.1208,  2.5757, -0.0650,  1.4334,  0.6059,\n",
       "           0.8823],\n",
       "         [ 1.9290, -0.4476,  0.5116,  0.0000,  0.4895, -1.8389,  1.9962,\n",
       "           1.3165],\n",
       "         [ 0.0000, -0.8013, -0.0000,  1.7064,  0.3865,  1.1128, -0.6196,\n",
       "          -0.2335],\n",
       "         [-2.3316, -2.2084,  0.3167,  1.1464,  0.4992,  1.3762,  0.5085,\n",
       "          -0.0526]]], device='cuda:0', grad_fn=<NativeDropoutBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_emb(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
